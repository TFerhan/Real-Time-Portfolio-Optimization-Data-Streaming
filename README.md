# Real-Time-Portfolio-Optimization-Data-Streaming

This project implements a real-time portfolio optimization pipeline, specifically targeting assets listed on the Casablanca Stock Exchange. It’s designed as a fully backend system focused on data flow and analytics, with no frontend interface. The core objective is to stream real-time stock prices for 10 selected assets and dynamically adjust portfolio allocations based on calculated risk-return metrics.

Weights are initialized using the Black-Litterman model. As new prices are streamed, the system updates the covariance matrix and expected returns using the Welford online algorithm. These updated statistics are used to compute the Sharpe ratio, which serves as the primary optimization trigger. When a defined threshold is crossed, a reallocation process is initiated to optimize performance. The loop continuously runs to minimize latency and manage risk exposure, assuming a risk-aversion coefficient of one (configurable).

Due to lack of access to the official real-time API from the Casablanca Stock Exchange (which is regulated and paid), the project simulates real-time data using a scheduler-based polling approach on their public API. While the Moroccan market lacks high-frequency data like larger exchanges (e.g. NSE, LSE), the simulation is sufficient for system behavior testing and integration validation.

Data ingestion is handled through a structured AVRO schema, serialized and transmitted via Kafka. Each asset price is encapsulated in a schema-compliant class to ensure compatibility across consumers. The AVRO schema definitions are stored in the Kafka component under the `resources` directory. Kafka topics are partitioned to enable parallel data processing and distributed scalability. Configuration files for the Kafka producer are located under the `Producer` directory, and all relevant Kafka parameters are documented there.

Additional data sources, including index-level and daily price APIs, are used to support the initial weight allocation and can also be integrated into the live stream. Once prices are flowing, the same pipeline infrastructure is applied to weights and portfolio statistics to establish the system state before switching to streaming-only for prices.

In the second phase, real-time prices are consumed by Apache Flink. Flink applies watermarks to each message and operates using event time semantics to ensure accurate alignment with trade timestamps, not ingestion time. Consumers are configured to use the latest offsets to avoid replaying historical data. Once consumers and watermarks are initialized, Flink maps incoming data by portfolio ID (for weights and statistics) and by ticker (for prices).

The log returns are computed in a windowed aggregation function, using intervals to group streamed prices. A small epsilon is added to the base price to account for simulation noise. This logic is encapsulated in `LogReturnWindowFunction` within the Flink component's utils folder.

Once log returns are available, they are joined with existing portfolio statistics to update covariance and expected return values. This is done by merging the return stream with the keyed portfolio stats stream using Flink’s stateful processing engine. A value state holds the portfolio stats object, which is updated incrementally using the Welford algorithm. The state can be persisted in RocksDB to allow checkpoint-based recovery (optional and not implemented here). The core logic for this operation resides in the `PortfolioUpdate` function, enabling consistent updates in near real-time.

Updated portfolio stats are published to a new Kafka topic. From this point, the system has all necessary components—current weights and updated statistics—to compute portfolio metrics. A dedicated coprocessing function, `PortfolioMetricsFunction`, consumes both streams and maintains three separate states: weights, stats, and metrics. Whenever an update occurs in either weights or stats, the Sharpe ratio is recalculated using the standard formula: (expected return - risk-free rate) / standard deviation (derived from covariance). This provides a real-time metric reflecting portfolio efficiency.

To make the Sharpe ratio actionable, a threshold is defined. When breached, a notification or downstream trigger is initiated. This is especially relevant for scenarios requiring full automation (e.g. alerts while traveling). The final processing layer is handled in Python, where the Kafka sink containing updated portfolio metrics is consumed.

The Python component includes a consumer class for weights, portfolio statistics, and metrics. Once the Sharpe ratio crosses the predefined threshold, the system invokes PyPortfolioOpt to re-optimize the asset weights using mean-variance optimization with a regularization term to promote diversification. The new weights are then published back to the Kafka weights topic, closing the loop and maintaining a fully reactive system capable of minimizing risk at fine-grained intervals.

All Kafka topics—including weights and metrics—are visualized in real-time using InfluxDB and Grafana. Grafana supports custom alerts (e.g. email notifications) when specific conditions are met. Final outputs can be optionally persisted to time-series databases such as TimeScaleDB (now TigerDB) or MongoDB Timeseries for historical analysis and auditability.

From a DevOps perspective, Kafka and Flink components are containerized and orchestrated via Kubernetes to ensure secure communication and high availability. Grafana and InfluxDB are also deployed as managed services. Although cloud deployment has not been finalized, the project is designed with future AWS deployment in mind, leveraging AWS Managed Kafka, Flink, and EC2-based configurations.
