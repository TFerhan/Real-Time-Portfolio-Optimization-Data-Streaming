# Real-Time-Portfolio-Optimization-Data-Streaming

This project aims to optimize in real time a portfolio of assets, a use case for the Casablanca Stock Exchange precisely. It’s not a web app, just fully backend to visualization. The goal was to stream stock prices in real time of 10 assets, initialized weights by the Black-Litterman model. The resulting covariance matrix and expected returns would be updated each time a new stock price got produced by the Welford online algorithm so as to compute the new Sharpe ratio. That would be a metric to see if an optimization or reallocation of weights must be held, given a threshold, and the whole keeps repeating itself to reduce latency and save the system from losing on risk. Here it's assumed a neutral risk-averse person of one, you can modify it anyway. This project is just a use case from scratch, giving an API even if the Casablanca Stock Exchange doesn't allow personals to use their WebSocket or real-time API as it’s paid and regulated. Managed to use their public website API, simulating real time with a scheduler of every 2 ms, even if the Moroccan market isn’t very voluminous or liquid to simulate the real real-time, not like big markets like NSE or LSE, so it’s just a simulation.

So the API gets the prices and, using AVRO Schema, sends and produces the prices in a structured way as it must be sent in bytes and deserialized back to a formal class. You may find the AVRO schema corresponding to each class in the Kafka component folder under the resources folder. After producing that data into a Kafka topic, we set in configuration the number of partitions to allow parallelization or, in other terms, distribution—that is the main objective of this whole project. You may also find the whole configuration in the Producer folder in the Kafka folder. You can check the documentation of Kafka to understand each configuration.

Included other functions or APIs of index prices and daily prices API, those were used to calculate and initialize the weights in the first place, but they also may be integrated in the pipeline in real time. Leaving that, after producing the prices, same thing we can do with the weights and the portfolio stats as the covariance and the expected return, just to initialize the pipeline. After that, solely the stock prices get streamed.

In the second phase, the prices get consumed by the second component, Flink, that sets watermarks upon each message that is consumed and also processes the whole thing as event time (even if in the latest versions of Flink they removed the processing time), so that the time is consistent with the time of the actual trade and not when it’s produced by the system. Also setting the offset to latest to consume just the latest messages and not be confused with earliest. After setting up each consumer, we set the watermarks, then the datasource objects. For the weight and portfolio stats sources, we map by the portfolio ID, as each portfolio has a set of weights and its statistics. For the prices, we map by the ticker. After mapping each of those last, we perform a windowed log return job. That being said, what it does is taking a window time like an interval of time after stacking streamed prices into a window of time so that we can calculate the return, or in our case the log return. Adding a random standard epsilon to the starting price is done as we are just simulating real-time stock market to get consistent results. You may find that function in the utils folder of the Flink component, `LogReturnWindowFunction`.

After getting those returns, we can now update the portfolio stats (covariance and expected returns). To use that, we need to merge the two streaming jobs, the log return stream and connect it with the keyed portfolio stats stream. After setting the context, we can now enter the principal function of Flink, which is the stateful distributed processing. Kafka only streams messages (it may manipulate but is limited somehow). Using stateful streaming processing, we can use a value state variable of portfolio stats to update this last one with each new log return and it stays in the full state and can also be saved in RocksDB as a checkpoint, so in case of a failure you can continue from that checkpoint (not implemented here). The main utility of this function `PortfolioUpdate` is that it can merge two upcoming streams. In this business case, updating the portfolio stats using the Welford online algorithm to update the covariance and the mean returns in real time and saving them back on the same state, so we get in a nearly consistent, reliable time the backbone to optimize the portfolio.

That being said, we get those updated portfolio stats back in a new topic of new portfolio stats, so in a new Kafka sink. Now that we have all, from the weights to the stats, we can compute the portfolio metrics. As earlier, we connect those last two outcomes with a coprocessing function job, `PortfolioMetricsFunction`. In this function, we use three value states of weights, stats, and metrics so that whenever one of the streamed objects gets in—like if weight comes—the metrics get updated; if the stats get changed, the metrics get updated. We use the classical formula to compute the Sharpe ratio: expected return minus risk-free divided by risk. Risk is computed by raising the square root of the standard deviation calculated from the covariance to finally get in real time the updated Sharpe ratio.

Getting that last value doesn’t bring us much value unless we interpret it—meaning, unless setting a threshold to give it meaning of which is good or bad. Then what? Here it breaches a threshold—a notification? You may be in travel or whatever and you want all automated for you. That is when the last component comes into place. Using a Python layer, we stream the resulting Sharpe ratio and the other metrics into a new Portfolio Metric Kafka sink to be later consumed by Python.

Now, creating a consumer class in Python to consume the weights, the portfolio metrics, and the portfolio stats, we set the existing consumer configurations. Then, using PyPortfolioOpt mean-variance with a small regularization to balance and diversify the weights upon each asset. If the Sharpe ratio consumed hits the threshold, a reallocation of weights takes place and produces the new weights to the weights topic, and the whole process takes a loop, mitigating risk to the slightest second without losing any profit.

All those topics—weights and portfolio metrics—are being visualized by InfluxDB and Grafana. This last one may notify in your email for the threshold or the customized alert. In the end, you may save and produce back the data into TimeScaleDB (now named TigerDb, a Postgres DB for time series) or MongoDB time series for consistency.

For the DevOps side, the Kafka and Flink components are managed with Kubernetes to secure connection and also Grafana and InfluxDB. For cloud deployment, it’s not yet managed but it will be implemented in the future and I will commit the setup here, mainly on AWS using Managed Kafka and Flink services and some other EC2 configurations.
